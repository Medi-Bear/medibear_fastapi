import os
import joblib
import pandas as pd
import json
from dotenv import load_dotenv
from groq import Groq


APP_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
MODEL_DIR = os.path.join(APP_DIR, "models", "calorie")

MODEL_PATH = os.path.join(MODEL_DIR, "calorie_model_hgbr.pkl")
ENCODER_PATH = os.path.join(MODEL_DIR, "encoder_dict.json")

# ëª¨ë¸ ë¡œë“œ
model = joblib.load(MODEL_PATH)

with open(ENCODER_PATH, 'r') as f:
    encoder_dict = json.load(f)

# 2) í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# 3) Groq client ìƒì„±
groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))


# ì¹¼ë¡œë¦¬ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_calories(duration_minutes: float, 
    weight_kg: float, activity_type: str, bmi: float, gender: str):

    # ì…ë ¥ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    data = pd.DataFrame({
        "duration_minutes": [duration_minutes],
        "weight_kg": [weight_kg],
        "activity_type": [activity_type],
        "bmi": [bmi],
        "gender": [gender]
    })
    
    # ë²”ì£¼í˜• ì¸ì½”ë”©: activity_type + gender
    for col in ["activity_type", "gender"]:

        input_val = data[col][0]

        # ì •ìƒì ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¼ë©´ ê·¸ëŒ€ë¡œ ë§¤í•‘
        if input_val in encoder_dict[col]:
            data[col] = [encoder_dict[col][input_val]]
        else:
            print(f"'{input_val}'ì€ í•™ìŠµ ë°ì´í„°ì— ì—†ìŒ â†’ Unknown ì²˜ë¦¬")
            data[col] = [encoder_dict[col]["Unknown"]]

    prediction = model.predict(data)[0]

    return round(float(prediction), 2)


# ğŸ”¥ Groq LLM ë¶„ì„ í•¨ìˆ˜
def llm_anaylze_calorie(request):
    
    user = request.member
    latest = request.latest
    logs = request.logs
    # DBì—ì„œ ê°€ì§€ê³ ì˜¨ ìš´ë™íƒ€ì…, ì‹œê°„, ì†Œëª¨í•œ ì¹¼ë¡œë¦¬, ëª¸ë¬´ê²Œ ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    log_text = "\n".join(
        f"{l.activityType}/{l.durationMinutes}ë¶„/{l.caloriesBurned}kcal/{l.weightKg}kg"
        for l in logs
    )

    prompt = f"""
        ë„ˆëŠ” ìµœê³ ì˜ í—¬ìŠ¤ ì½”ì¹˜ì•¼. ì•„ë˜ëŠ” ì‚¬ìš©ìì˜ ìš´ë™ ê¸°ë¡ì´ë‹ˆê¹Œ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•´ì¤˜.
    ë‹¨, ìš´ë™ ê¸°ë¡(raw data)ì€ ë¶„ì„ì—ëŠ” ì‚¬ìš©í•˜ë˜ **ìµœì¢… ë‹µë³€ì—ëŠ” ê·¸ëŒ€ë¡œ ë…¸ì¶œí•˜ì§€ ë§ˆë¼.**
    
    ì‚¬ìš©ì ì •ë³´:
    - ì´ë¦„: {user.name}
    - ì„±ë³„: {user.gender}
    - ìµœì‹  í‚¤: {latest.heightCm} cm
    - ìµœì‹  ëª¸ë¬´ê²Œ: {latest.weightKg} kg
    - ìµœì‹  BMI: {latest.bmi}
    
        ì•„ë˜ëŠ” ìš´ë™ ê¸°ë¡ ë°ì´í„°(LLM ë¶„ì„ìš©):
    ---
    {log_text}
    ---
    ìœ„ ë°ì´í„°ëŠ” ë¶„ì„ì—ë§Œ ì‚¬ìš©í•˜ê³  ë‹µë³€ì— ë…¸ì¶œí•˜ì§€ ë§ˆë¼.


    ì‘ë‹µ:
    1) ìš´ë™ íŒ¨í„´ ìš”ì•½ 
    2) ì¹¼ë¡œë¦¬ ì†Œëª¨ ì¶”ì„¸ 
    3) 15ì¼ í›„ ì˜ˆìƒ ëª¸ë¬´ê²Œ ë³€í™” (ì¹¼ë¡œë¦¬ ì†Œëª¨ëŸ‰ ê¸°ë°˜)
    - ì œê³µëœ ìš´ë™ ê¸°ë¡ìœ¼ë¡œ ê³„ì‚°ëœ 'ìš´ë™ ì†Œëª¨ ì¹¼ë¡œë¦¬'ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœ ì˜ˆì¸¡í•  ê²ƒ
    - ë‚˜ì´, BMR, TDEE, ì„­ì·¨ ì¹¼ë¡œë¦¬ ë“± ì œê³µë˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
    - ê¸°ì´ˆëŒ€ì‚¬ëŸ‰ ê³µì‹(Mifflin-St Jeor ë“±)ì„ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
    - ë‹¨, ìš´ë™ ì†Œëª¨ ì¹¼ë¡œë¦¬ë§Œìœ¼ë¡œ ì¶”ì •í•œ ë‹¨ìˆœ ê³„ì‚°ì€ ë°˜ë“œì‹œ ìˆ˜í–‰í•  ê²ƒ
    - ì‹ì‚¬ ì •ë³´ê°€ ì—†ì–´ ì •í™•í•œ ì²´ì¤‘ ë³€í™”ëŠ” ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤ëŠ” ë‹¨ì„œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•  ê²ƒ
   
    í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì¤˜.

    ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ "ìš”ì•½:"ì„ í¬í•¨í•´ 1ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:
    ìš”ì•½: ~~~
    
    """

    response = groq_client.chat.completions.create(
        messages=[{"role": "user", "content": prompt}],
        model="openai/gpt-oss-120b",
        temperature=0.3
    )

    advice = response.choices[0].message.content.strip()

    # "ìš”ì•½:" ë¶€ë¶„ë§Œ íŒŒì‹±
    summary = None
    if "ìš”ì•½:" in advice:
        summary = advice.split("ìš”ì•½:")[-1].strip()
    else:
        summary = advice[:200]  # fallback

    return {
        "prompt": prompt,
        "advice": advice,
        "summary": summary
    }
