# exercise_llm.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from datetime import datetime
from typing import Dict, Any, Optional, List
from pymongo import MongoClient
import numpy as np
import asyncio
import os
import re

# ===== Embedding (384-d fixed) =====
from sentence_transformers import SentenceTransformer
EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")

def embed(text: str) -> List[float]:
    return embed_model.encode(text, normalize_embeddings=True).tolist()

VECTOR_DIM = len(embed_model.encode("dim"))

# ===== LLM (Qwen GGUF ëª¨ë¸ ì‚¬ìš©) =====
from llama_cpp import Llama
MODEL_PATH = "../../models/exercise_models/qwen2.5-3b-instruct-q4_k_m.gguf"   # âœ… ë„¤ê°€ ë‹¤ìš´ë°›ì€ ëª¨ë¸ ì´ë¦„ ê·¸ëŒ€ë¡œ

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=max(1, (os.cpu_count() or 2) - 1),
    n_batch=128,
    logits_all=False,
    verbose=False,
    chat_format="chatml",
)

# ===== FastAPI =====
app = FastAPI(title="MediBear LLM Server (Local Mongo + RAG)")

# ===== MongoDB =====
client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=500)
db = client["ai_coach"]
chat_col = db["chat_history"]
profile_col = db["profile"]

# ===== Input Models =====
class ChatInput(BaseModel):
    user_id: str
    message: str

class ChatWithAnalysisInput(BaseModel):
    user_id: str
    message: str
    analysis: Dict[str, Any]

# ===== Utilities =====
def cosine_similarity(a, b) -> float:
    a, b = np.asarray(a), np.asarray(b)
    na, nb = np.linalg.norm(a), np.linalg.norm(b)
    return float(np.dot(a, b) / (na * nb)) if na * nb != 0 else 0.0

def safe_get_vec(doc) -> Optional[List[float]]:
    vec = doc.get("embedding") or doc.get("vector")
    if isinstance(vec, list) and len(vec) == VECTOR_DIM:
        return vec
    return None

def clean_text_korean_only(text: str) -> str:
    return re.sub(r"[^ê°€-í£0-9\s\.\,\?\!]", "", text)


# ===== RAG =====
def build_rag_context(user_id: str, user_msg: str, topk: int = 3, mode: str = "auto") -> str:
    qvec = embed(user_msg)

    if mode == "exercise":
        query = {"user_id": user_id, "type": "exercise"}
    elif mode == "general":
        query = {"user_id": user_id, "type": "general"}
    else:
        query = {"user_id": user_id}

    history = list(
        chat_col.find(query)
               .sort("timestamp", -1)
               .limit(60)
    )

    scored = []
    for h in history:
        vec = safe_get_vec(h)
        if vec:
            cleaned = clean_text_korean_only(h["message"])
            if cleaned.strip():
                scored.append((cosine_similarity(qvec, vec), cleaned))

    if not scored:
        return ""

    scored.sort(key=lambda x: x[0], reverse=True)
    return "\n".join([f"User said before: {msg}" for _, msg in scored[:topk]])


# ===== Prompt ì •ì˜ =====
SYSTEM_PROMPT_EXERCISE = (
    "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì´ë‹¤.\n"
    "ì‚¬ìš©ìê°€ ì–´ë–¤ ìš´ë™ì„ í•˜ê³  ìˆëŠ”ì§€ ë°˜ë“œì‹œ ì²« ì¤„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•œë‹¤.\n"
    "ë¶„ì„ ë°ì´í„°ëŠ” ì°¸ê³ í•˜ì§€ë§Œ ìˆ˜ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
    "ìˆ«ì, ê°ë„, cm, %, Â° ë“± ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€.\n"
    "í•œì, ì˜ì–´, ì „ë¬¸ìš©ì–´, ê³¼í•™ì  í‘œí˜„ ê¸ˆì§€.\n"
    "ìš´ë™ ë™ì‘ì— ëŒ€í•œ ëŠë‚Œ, ê¸´ì¥, í˜ì˜ íë¦„, ì²´ì¤‘ ë¶„ë°° ì¤‘ì‹¬ìœ¼ë¡œ í”¼ë“œë°±í•œë‹¤.\n\n"
    "ì¶œë ¥ í˜•ì‹:\n"
    "â‘  ì§€ê¸ˆ í•˜ê³  ìˆëŠ” ìš´ë™ ì´ë¦„ + ìì„¸ ëŠë‚Œ ìš”ì•½ (1~2ë¬¸ì¥)\n"
    "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
    "â‘¢ ê°œì„ í•  ì  (â€¢ ë¶ˆë¦¿ 2~3ê°œ)\n"
    "â‘£ ì½”ì¹­ í (â€¢ ë¶ˆë¦¿ 3~5ê°œ, ëª…ë ¹í˜• 4~8ê¸€ì)\n"
    "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥)\n"
    "(1ë¬¸ì¥) ì´ëŸ°ê±°ëŠ” ì–¸ê¸‰ ê¸ˆì§€\n"
)


SYSTEM_PROMPT_GENERAL = (
    "ë„ˆëŠ” í•œêµ­ì–´ í—¬ìŠ¤ì¼€ì–´ ìƒë‹´ AIë‹¤.\n"
    "ê³µê° + ì§§ê³  ë‹¨í˜¸ + ë”°ëœ»í•œ í†¤.\n"
    "ìš´ë™ í…œí”Œë¦¿(â‘ ~â‘¤) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
)


# ===== LLM Wrapper =====
async def llm_generate(messages):
    def _run():
        out = llm.create_chat_completion(
            messages=messages,
            temperature=0.2,
            top_p=0.9,
            repeat_penalty=1.12,
            max_tokens=600,
        )
        return out["choices"][0]["message"]["content"].strip()
    return await asyncio.to_thread(_run)


# ===== Persona ìš”ì•½ =====
async def update_persona_background(user_id: str):
    chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(15))
    if not chats:
        return
    text = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])
    summary = await llm_generate([
        {"role": "system", "content": "ì‚¬ìš©ìì˜ í†µì¦ ê²½í–¥/ìš´ë™ ìŠµê´€/ë§íˆ¬ë¥¼ 5ì¤„ë¡œ ìš”ì•½í•˜ë¼."},
        {"role": "user", "content": text},
    ])
    profile_col.update_one({"user_id": user_id}, {"$set": {"persona": summary}}, upsert=True)


# ===== ë‹µë³€ ìƒì„± =====
async def generate_answer(user_id: str, user_msg: str, analysis: Dict[str, Any]):
    persona = profile_col.find_one({"user_id": user_id}) or {}
    persona_text = persona.get("persona", "")

    # âœ… ì—¬ê¸° ì¶”ê°€: ìš´ë™ ë¶„ì„ JSON í‚¤ ë§ì¶°ì£¼ê¸°
    if analysis and "exercise" in analysis and "detected_exercise" not in analysis:
        analysis["detected_exercise"] = analysis["exercise"]

    # âœ… ìš´ë™ ì—¬ë¶€ íŒë³„ (ì´ì œ ì •ìƒ ë™ì‘)
    is_exercise = bool(analysis and analysis.get("detected_exercise"))

    # âœ… ìš´ë™ëª… í…ìŠ¤íŠ¸ ì•ì— ë¶™ì´ê¸°
    if is_exercise:
        exercise_name = analysis["detected_exercise"]
        user_msg = f"{exercise_name} ìš´ë™ ì¤‘: {user_msg}"

    if is_exercise:
        system_prompt = SYSTEM_PROMPT_EXERCISE
        rag = build_rag_context(user_id, user_msg, mode="exercise")
    else:
        system_prompt = SYSTEM_PROMPT_GENERAL
        rag = build_rag_context(user_id, user_msg, mode="general")

    user_prompt = (rag + "\n\n" + user_msg) if rag else user_msg

    messages = [
        {"role": "system", "content": system_prompt + ("\n[ì‚¬ìš©ì ìš”ì•½]\n" + persona_text if persona_text else "")},
        {"role": "user", "content": user_prompt},
    ]

    return await llm_generate(messages)




# ===== ì €ì¥ =====
def save_chat(user_id, message, response, embedding, analysis):
    chat_col.insert_one({
        "user_id": user_id,
        "message": message,
        "response": response,
        "embedding": embedding,
        "analysis": analysis or {},
        "timestamp": datetime.now(),
        "type": "exercise" if (analysis and analysis.get("detected_exercise")) else "general",
    })


# ===== Endpoints =====
@app.post("/chat")
async def chat_plain(data: ChatInput, background_tasks: BackgroundTasks):
    qvec = embed(data.message)
    answer = await generate_answer(data.user_id, data.message, analysis={})
    save_chat(data.user_id, data.message, answer, qvec, {})
    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)
    return {"answer": answer}

@app.post("/chat_with_analysis")
async def chat_with_analysis(data: ChatWithAnalysisInput, background_tasks: BackgroundTasks):

    # âœ… message ë¹„ì–´ìˆìœ¼ë©´ ìë™ ìƒì„±
    user_msg = data.message if data.message and data.message.strip() else "ìš´ë™ ìì„¸ í”¼ë“œë°± ìš”ì²­"

    qvec = embed(user_msg)

    print("\n")
    print(data.analysis)
    print("\n")

    answer = await generate_answer(data.user_id, user_msg, data.analysis)

    save_chat(data.user_id, user_msg, answer, qvec, data.analysis)

    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)

    return {"answer": answer}


# # exercise_llm.py
# from fastapi import FastAPI, BackgroundTasks
# from pydantic import BaseModel
# from datetime import datetime
# from typing import Dict, Any, Optional, List
# from pymongo import MongoClient
# import numpy as np
# import asyncio
# import os

# # ===== Embedding (384-d fixed) =====
# from sentence_transformers import SentenceTransformer
# EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"
# embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")

# def embed(text: str) -> List[float]:
#     return embed_model.encode(text, normalize_embeddings=True).tolist()

# VECTOR_DIM = len(embed_model.encode("dim"))

# # ===== LLM (Qwen 1.5B GGUF) =====
# from llama_cpp import Llama
# MODEL_PATH = "../../models/exercise_models/qwen2.5-1.5b-instruct-q4_k_m.gguf"

# llm = Llama(
#     model_path=MODEL_PATH,
#     n_ctx=2048,
#     n_threads=max(1, (os.cpu_count() or 2) - 1),
#     n_batch=128,
#     logits_all=False,
#     verbose=False,
#     chat_format="chatml",
# )

# # ===== FastAPI =====
# app = FastAPI(title="MediBear LLM Server (Local Mongo + RAG)")

# # ===== MongoDB =====
# client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=500)
# db = client["ai_coach"]
# chat_col = db["chat_history"]
# profile_col = db["profile"]


# # ===== Input Models =====
# class ChatInput(BaseModel):
#     user_id: str
#     message: str

# class ChatWithAnalysisInput(BaseModel):
#     user_id: str
#     message: str
#     analysis: Dict[str, Any]

# # ===== Utilities =====
# def cosine_similarity(a, b) -> float:
#     a, b = np.asarray(a), np.asarray(b)
#     na, nb = np.linalg.norm(a), np.linalg.norm(b)
#     return float(np.dot(a, b) / (na * nb)) if na * nb != 0 else 0.0

# def safe_get_vec(doc) -> Optional[List[float]]:
#     vec = doc.get("embedding") or doc.get("vector")
#     if isinstance(vec, list) and len(vec) == VECTOR_DIM:
#         return vec
#     return None


# # ===== RAG (ìš´ë™ ëŒ€í™”ë§Œ ê²€ìƒ‰) =====
# def build_rag_context(user_id: str, user_msg: str, topk: int = 3) -> str:
#     qvec = embed(user_msg)

#     # ğŸ”¥ ë³€ê²½: ìš´ë™ ê¸°ë¡ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
#     history = list(
#         chat_col.find({"user_id": user_id, "type": "exercise"})
#                 .sort("timestamp", -1)
#                 .limit(50)
#     )

#     scored = []
#     for h in history:
#         vec = safe_get_vec(h)
#         if vec:
#             scored.append((cosine_similarity(qvec, vec), h["message"], h["response"]))

#     if not scored:
#         return ""

#     scored.sort(key=lambda x: x[0], reverse=True)
#     return "\n---\n".join([f"User: {u}\nAI: {a}" for _, u, a in scored[:topk]])


# # ===== Prompt ë³€í™˜ =====
# SYSTEM_PROMPT_EXERCISE = (
#     "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì´ë‹¤. ë¶„ì„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê° ì¤‘ì‹¬ í”¼ë“œë°±ë§Œ ì œê³µí•œë‹¤.\n"
#     "ìˆ«ì, ê°ë„, cm, %, Â° ë“± ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€.\n"
#     "ì¶œë ¥ í˜•ì‹ì€ ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥¸ë‹¤:\n"
#     "â‘  ìì„¸ ëŠë‚Œ ìš”ì•½ (2ë¬¸ì¥)\n"
#     "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
#     "â‘¢ ê°œì„ í•  ì  (â€¢ ë¶ˆë¦¿ 2~3ê°œ)\n"
#     "â‘£ ì½”ì¹­ í (â€¢ ë¶ˆë¦¿ 3~5ê°œ, 4~8ê¸€ì ëª…ë ¹í˜•)\n"
#     "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥)\n"
# )

# SYSTEM_PROMPT_GENERAL = (
#     "ë„ˆëŠ” í•œêµ­ì–´ í—¬ìŠ¤ì¼€ì–´ ìƒë‹´ AIë‹¤.\n"
#     "ê³µê° + ê°„ë‹¨ëª…ë£Œ + ë”°ëœ»í•œ í†¤ìœ¼ë¡œ ë‹µí•œë‹¤.\n"
#     "ìš´ë™ í”¼ë“œë°± í˜•ì‹(â‘ ~â‘¤)ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
# )


# # ===== Persona ìš”ì•½ =====
# async def update_persona_background(user_id: str):
#     chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(12))
#     if not chats:
#         return
#     text = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])
#     summary = await llm_generate([
#         {"role": "system", "content": "ì‚¬ìš©ìì˜ ìš´ë™ ëª©í‘œ/ëª¸ ìƒíƒœ/ë§íˆ¬ë¥¼ 5ì¤„ë¡œ ìš”ì•½í•´ë¼."},
#         {"role": "user", "content": text},
#     ])
#     profile_col.update_one({"user_id": user_id}, {"$set": {"persona": summary}}, upsert=True)


# # ===== LLM í˜¸ì¶œ =====
# async def llm_generate(messages):
#     def _run():
#         out = llm.create_chat_completion(
#             messages=messages,
#             temperature=0.5,
#             top_p=0.9,
#             repeat_penalty=1.12,
#             max_tokens=600,
#         )
#         return out["choices"][0]["message"]["content"].strip()
#     return await asyncio.to_thread(_run)


# # ===== ë‹µë³€ ìƒì„± (í•µì‹¬ ë¡œì§) =====
# async def generate_answer(user_id: str, user_msg: str, analysis: Dict[str, Any]):
#     persona = profile_col.find_one({"user_id": user_id}) or {}
#     persona_text = persona.get("persona", "")

#     is_exercise = bool(analysis and analysis.get("detected_exercise"))

#     if is_exercise:
#         system_prompt = SYSTEM_PROMPT_EXERCISE
#         user_prompt = build_rag_context(user_id, user_msg) + "\n\n" + user_msg
#     else:
#         system_prompt = SYSTEM_PROMPT_GENERAL
#         user_prompt = user_msg

#     messages = [
#         {"role": "system", "content": system_prompt + ("\n[ì‚¬ìš©ì ìš”ì•½]\n" + persona_text if persona_text else "")},
#         {"role": "user", "content": user_prompt},
#     ]

#     return await llm_generate(messages)


# # ===== ì €ì¥ (ìš´ë™/ì¼ë°˜ ìë™ íƒœê·¸) =====
# def save_chat(user_id, message, response, embedding, analysis):
#     chat_col.insert_one({
#         "user_id": user_id,
#         "message": message,
#         "response": response,
#         "embedding": embedding,
#         "analysis": analysis or {},
#         "timestamp": datetime.now(),
#         "type": "exercise" if (analysis and analysis.get("detected_exercise")) else "general",  # ğŸ”¥ ë³€ê²½
#     })


# # ===== Endpoints =====
# @app.post("/chat")
# async def chat_plain(data: ChatInput, background_tasks: BackgroundTasks):
#     qvec = embed(data.message)
#     answer = await generate_answer(data.user_id, data.message, analysis={})
#     save_chat(data.user_id, data.message, answer, qvec, {})
#     if chat_col.count_documents({"user_id": data.user_id}) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)
#     return {"answer": answer}

# @app.post("/chat_with_analysis")
# async def chat_with_analysis(data: ChatWithAnalysisInput, background_tasks: BackgroundTasks):
#     qvec = embed(data.message)
#     answer = await generate_answer(data.user_id, data.message, data.analysis)
#     save_chat(data.user_id, data.message, answer, qvec, data.analysis)
#     if chat_col.count_documents({"user_id": data.user_id}) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)
#     return {"answer": answer}



# from fastapi import FastAPI, BackgroundTasks
# from pydantic import BaseModel
# from datetime import datetime
# from pymongo import MongoClient
# from sentence_transformers import SentenceTransformer
# import numpy as np
# import asyncio
# import os
# from llama_cpp import Llama

# app = FastAPI()

# # ---------------- MongoDB ----------------
# client = MongoClient("mongodb://localhost:27017")
# db = client["ai_coach"]
# chat_col = db["chat_history"]
# profile_col = db["profile"]

# # ---------------- Embedding Model ----------------
# embed_model = SentenceTransformer("intfloat/multilingual-e5-small", device="cpu")

# # ---------------- LLM ----------------
# MODEL_PATH = "../../models/exercise_models/qwen2.5-1.5b-instruct-q4_k_m.gguf"
# llm = Llama(
#     model_path=MODEL_PATH,
#     n_ctx=1024,
#     n_threads=8,
#     n_batch=128,
#     logits_all=False,
#     verbose=False,
#     chat_format="chatml"
# )

# class ChatInput(BaseModel):
#     user_id: str
#     message: str

# def cosine_similarity(a, b):
#     a, b = np.array(a), np.array(b)
#     if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
#         return 0.0
#     return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


# async def generate_async(user_msg: str, persona: str, context_text: str):
#     messages = [
#         {"role": "system",
#          "content": (
#              "ë‹¹ì‹ ì€ ê°œì¸ ë§ì¶¤í˜• ê±´ê°•/ìš´ë™ ìƒë‹´ ì½”ì¹˜ AIì…ë‹ˆë‹¤.\n"
#              "ì‚¬ìš©ìì˜ ì§€ë‚œ ëŒ€í™” ë‚´ìš©(persona ìš”ì•½ + ìµœê·¼ ëŒ€í™” context)ì„ ì°¸ê³ í•˜ì—¬ "
#              "ì‚¬ìš©ìì˜ ìƒíƒœì™€ ê°ì •, ìŠµê´€ì„ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ëŒ€í™”ë¥¼ í•˜ì„¸ìš”."
#          )},
#         {"role": "user",
#          "content": f"[ì‚¬ìš©ì ìš”ì•½ ì •ë³´]\n{persona}\n\n[ìµœê·¼ ê´€ë ¨ ëŒ€í™”]\n{context_text}\n\n[í˜„ì¬ ì§ˆë¬¸]\n{user_msg}"}
#     ]

#     def _run():
#         out = llm.create_chat_completion(
#             messages=messages,
#             temperature=0.35,
#             top_p=0.9,
#             max_tokens=240,
#             stop=["</s>", "<|im_end|>"]
#         )
#         return out["choices"][0]["message"]["content"].strip()

#     return await asyncio.to_thread(_run)


# async def update_persona_background(user_id: str):
#     chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(10))
#     text_block = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])

#     messages = [
#         {"role": "system", "content": "ìµœê·¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê±´ê°•/ìš´ë™ íŠ¹ì§•ì„ 5ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”."},
#         {"role": "user", "content": text_block or "(ëŒ€í™”ì—†ìŒ)"}
#     ]

#     def _run():
#         out = llm.create_chat_completion(messages=messages, temperature=0.2, top_p=0.9, max_tokens=120)
#         return out["choices"][0]["message"]["content"].strip()

#     summary = await asyncio.to_thread(_run)

#     profile_col.update_one(
#         {"user_id": user_id},
#         {"$set": {"persona": summary, "updated_at": datetime.now()}},
#         upsert=True
#     )


# @app.post("/chat")
# async def chat_with_ai(data: ChatInput, background_tasks: BackgroundTasks):

#     # 1) ì…ë ¥ ë¬¸ì¥ ì„ë² ë”©
#     emb = embed_model.encode(data.message, normalize_embeddings=True)
#     user_vec = emb.tolist()

#     # 2) ìµœê·¼ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° + RAG (ìœ ì‚¬ë„ ìƒìœ„ 3ê°œ)
#     history = list(chat_col.find({"user_id": data.user_id}).sort("timestamp", -1).limit(10))

#     contexts = []
#     for h in history:
#         vec = h.get("embedding") or h.get("vector")
#         if not vec:
#             continue
#         if len(vec) != len(user_vec):
#             continue    # âœ… ì°¨ì› ë‹¤ë¥´ë©´ skip
#         sim = cosine_similarity(user_vec, vec)
#         contexts.append((sim, h["message"], h.get("response", "")))

#     if contexts:
#         contexts = sorted(contexts, key=lambda x: x[0], reverse=True)[:3]
#         context_text = "\n".join([f"User: {m}\nAI: {r}" for _, m, r in contexts])
#     else:
#         context_text = "\n".join([f"User: {h['message']}\nAI: {h['response']}" for h in history[:3]])

#     # 3) Persona ë¶ˆëŸ¬ì˜¤ê¸°
#     profile = profile_col.find_one({"user_id": data.user_id})
#     persona = profile["persona"] if profile else "íŠ¹ì§• ë¯¸íŒŒì•… ì‚¬ìš©ì"

#     # 4) LLM í˜¸ì¶œ
#     answer = await generate_async(data.message, persona, context_text)

#     # 5) ì €ì¥
#     chat_col.insert_one({
#         "user_id": data.user_id,
#         "message": data.message,
#         "response": answer,
#         "embedding": user_vec,
#         "timestamp": datetime.now()
#     })

#     # 6) Persona ì—…ë°ì´íŠ¸ëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ
#     if len(history) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)

#     return {"answer": answer, "persona_summary": persona}

