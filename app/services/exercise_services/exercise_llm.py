# # ===== Load .env =====
# # from dotenv import load_dotenv
# # load_dotenv()

# # exercise_llm.py
# from fastapi import FastAPI, BackgroundTasks
# from pydantic import BaseModel
# from datetime import datetime
# from typing import Dict, Any, Optional, List
# from pymongo import MongoClient
# import numpy as np
# import asyncio
# import os
# import re

# # ===== Embedding (384-d fixed) =====
# from sentence_transformers import SentenceTransformer
# EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"
# embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")

# def embed(text: str) -> List[float]:
#     return embed_model.encode(text, normalize_embeddings=True).tolist()

# VECTOR_DIM = len(embed_model.encode("dim"))

# # ===== LLM (Qwen GGUF ëª¨ë¸ ì‚¬ìš©) =====
# from llama_cpp import Llama
# # MODEL_PATH = "../../models/exercise_models/qwen2.5-3b-instruct-q4_k_m.gguf"   # âœ… ë„¤ê°€ ë‹¤ìš´ë°›ì€ ëª¨ë¸ ì´ë¦„ ê·¸ëŒ€ë¡œ  -> ìƒëŒ€ ê²½ë¡œ
# MODEL_PATH = r"C:\develop\medibear_fastapi\qwen2.5-3b-instruct-q4_k_m.gguf"


# llm = Llama(
#     model_path=MODEL_PATH,
#     # ì´ì „ : 2048
#     n_ctx=2048,
#     n_threads=max(1, (os.cpu_count() or 2) - 1),
#     n_batch=128,
#     logits_all=False,
#     verbose=False,
#     chat_format="chatml",
# )

# # ===== FastAPI =====
# app = FastAPI(title="MediBear LLM Server (Local Mongo + RAG)")

# # ===== MongoDB =====
# client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=500)
# db = client["ai_coach"]
# chat_col = db["chat_history"]
# profile_col = db["profile"]

# # ===== Input Models =====
# class ChatInput(BaseModel):
#     user_id: str
#     message: str

# class ChatWithAnalysisInput(BaseModel):
#     user_id: str
#     message: str
#     analysis: Dict[str, Any]

# # ===== Utilities =====
# def cosine_similarity(a, b) -> float:
#     a, b = np.asarray(a), np.asarray(b)
#     na, nb = np.linalg.norm(a), np.linalg.norm(b)
#     return float(np.dot(a, b) / (na * nb)) if na * nb != 0 else 0.0

# def safe_get_vec(doc) -> Optional[List[float]]:
#     vec = doc.get("embedding") or doc.get("vector")
#     if isinstance(vec, list) and len(vec) == VECTOR_DIM:
#         return vec
#     return None

# def clean_text_korean_only(text: str) -> str:
#     return re.sub(r"[^ê°€-í£0-9\s\.\,\?\!]", "", text)


# # ===== RAG =====
# def build_rag_context(user_id: str, user_msg: str, topk: int = 3, mode: str = "auto") -> str:
#     qvec = embed(user_msg)

#     if mode == "exercise":
#         query = {"user_id": user_id, "type": "exercise"}
#     elif mode == "general":
#         query = {"user_id": user_id, "type": "general"}
#     else:
#         query = {"user_id": user_id}

#     history = list(
#         chat_col.find(query)
#                .sort("timestamp", -1)
#                .limit(60)
#     )

#     scored = []
#     for h in history:
#         vec = safe_get_vec(h)
#         if vec:
#             cleaned = clean_text_korean_only(h["message"])
#             if cleaned.strip():
#                 scored.append((cosine_similarity(qvec, vec), cleaned))

#     if not scored:
#         return ""

#     scored.sort(key=lambda x: x[0], reverse=True)
#     return "\n".join([f"User said before: {msg}" for _, msg in scored[:topk]])


# # ===== Prompt ì •ì˜ =====
# SYSTEM_PROMPT_EXERCISE = (
#     "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì´ë‹¤.\n"
#     "ì‚¬ìš©ìê°€ ì–´ë–¤ ìš´ë™ì„ í•˜ê³  ìˆëŠ”ì§€ ë°˜ë“œì‹œ ì²« ì¤„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•œë‹¤.\n"
#     "ë¶„ì„ ë°ì´í„°ëŠ” ì°¸ê³ í•˜ì§€ë§Œ ìˆ˜ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
#     "ìˆ«ì, ê°ë„, cm, %, Â° ë“± ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€.\n"
#     "í•œì, ì˜ì–´, ì „ë¬¸ìš©ì–´, ê³¼í•™ì  í‘œí˜„ ê¸ˆì§€.\n"
#     "ìš´ë™ ë™ì‘ì— ëŒ€í•œ ëŠë‚Œ, ê¸´ì¥, í˜ì˜ íë¦„, ì²´ì¤‘ ë¶„ë°° ì¤‘ì‹¬ìœ¼ë¡œ í”¼ë“œë°±í•œë‹¤.\n\n"
#     "ì¶œë ¥ í˜•ì‹:\n"
#     "â‘  ì§€ê¸ˆ í•˜ê³  ìˆëŠ” ìš´ë™ ì´ë¦„ + ìì„¸ ëŠë‚Œ ìš”ì•½ (1~2ë¬¸ì¥)\n"
#     "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
#     "â‘¢ ê°œì„ í•  ì  (â€¢ ë¶ˆë¦¿ 2~3ê°œ)\n"
#     "â‘£ ì½”ì¹­ í (â€¢ ë¶ˆë¦¿ 3~5ê°œ, ëª…ë ¹í˜• 4~8ê¸€ì)\n"
#     "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥)\n"
#     "(1ë¬¸ì¥) ì´ëŸ°ê±°ëŠ” ì–¸ê¸‰ ê¸ˆì§€\n"
# )


# SYSTEM_PROMPT_GENERAL = (
#     "ë„ˆëŠ” í•œêµ­ì–´ í—¬ìŠ¤ì¼€ì–´ ìƒë‹´ AIë‹¤.\n"
#     "ê³µê° + ì§§ê³  ë‹¨í˜¸ + ë”°ëœ»í•œ í†¤.\n"
#     "ìš´ë™ í…œí”Œë¦¿(â‘ ~â‘¤) ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€.\n"
# )


# # ===== LLM Wrapper =====
# async def llm_generate(messages):
#     def _run():
#         out = llm.create_chat_completion(
#             messages=messages,
#             temperature=0.2,
#             top_p=0.9,
#             repeat_penalty=1.12,
#             # ì´ì „ : 600
#             max_tokens=256, 
#         )
#         return out["choices"][0]["message"]["content"].strip()
#     return await asyncio.to_thread(_run)


# # ===== Persona ìš”ì•½ =====
# async def update_persona_background(user_id: str):
#     chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(15))
#     if not chats:
#         return
#     text = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])
#     summary = await llm_generate([
#         {"role": "system", "content": "ì‚¬ìš©ìì˜ í†µì¦ ê²½í–¥/ìš´ë™ ìŠµê´€/ë§íˆ¬ë¥¼ 5ì¤„ë¡œ ìš”ì•½í•˜ë¼."},
#         {"role": "user", "content": text},
#     ])
#     profile_col.update_one({"user_id": user_id}, {"$set": {"persona": summary}}, upsert=True)


# # ===== ë‹µë³€ ìƒì„± =====
# async def generate_answer(user_id: str, user_msg: str, analysis: Dict[str, Any]):
#     persona = profile_col.find_one({"user_id": user_id}) or {}
#     persona_text = persona.get("persona", "")

#     # âœ… ì—¬ê¸° ì¶”ê°€: ìš´ë™ ë¶„ì„ JSON í‚¤ ë§ì¶°ì£¼ê¸°
#     if analysis and "exercise" in analysis and "detected_exercise" not in analysis:
#         analysis["detected_exercise"] = analysis["exercise"]

#     # âœ… ìš´ë™ ì—¬ë¶€ íŒë³„ (ì´ì œ ì •ìƒ ë™ì‘)
#     is_exercise = bool(analysis and analysis.get("detected_exercise"))

#     # âœ… ìš´ë™ëª… í…ìŠ¤íŠ¸ ì•ì— ë¶™ì´ê¸°
#     if is_exercise:
#         exercise_name = analysis["detected_exercise"]
#         user_msg = f"{exercise_name} ìš´ë™ ì¤‘: {user_msg}"

#     if is_exercise:
#         system_prompt = SYSTEM_PROMPT_EXERCISE
#         rag = build_rag_context(user_id, user_msg, mode="exercise")
#     else:
#         system_prompt = SYSTEM_PROMPT_GENERAL
#         rag = build_rag_context(user_id, user_msg, mode="general")

#     user_prompt = (rag + "\n\n" + user_msg) if rag else user_msg

#     messages = [
#         {"role": "system", "content": system_prompt + ("\n[ì‚¬ìš©ì ìš”ì•½]\n" + persona_text if persona_text else "")},
#         {"role": "user", "content": user_prompt},
#     ]

#     return await llm_generate(messages)




# # ===== ì €ì¥ =====
# def save_chat(user_id, message, response, embedding, analysis):
#     chat_col.insert_one({
#         "user_id": user_id,
#         "message": message,
#         "response": response,
#         "embedding": embedding,
#         "analysis": analysis or {},
#         "timestamp": datetime.now(),
#         "type": "exercise" if (analysis and analysis.get("detected_exercise")) else "general",
#     })


# # ===== Endpoints =====
# @app.post("/chat")
# async def chat_plain(data: ChatInput, background_tasks: BackgroundTasks):
#     qvec = embed(data.message)
#     answer = await generate_answer(data.user_id, data.message, analysis={})
#     save_chat(data.user_id, data.message, answer, qvec, {})
#     if chat_col.count_documents({"user_id": data.user_id}) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)
#     return {"answer": answer}

# @app.post("/chat_with_analysis")
# async def chat_with_analysis(data: ChatWithAnalysisInput, background_tasks: BackgroundTasks):

#     # âœ… message ë¹„ì–´ìˆìœ¼ë©´ ìë™ ìƒì„±
#     user_msg = data.message if data.message and data.message.strip() else "ìš´ë™ ìì„¸ í”¼ë“œë°± ìš”ì²­"

#     qvec = embed(user_msg)

#     print("\n")
#     print(data.analysis)
#     print("\n")

#     answer = await generate_answer(data.user_id, user_msg, data.analysis)

#     save_chat(data.user_id, user_msg, answer, qvec, data.analysis)

#     if chat_col.count_documents({"user_id": data.user_id}) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)

#     return {"answer": answer}


# ===== Load .env =====
# from dotenv import load_dotenv
# load_dotenv()



###############################################ì˜¬ë¼ë§ˆë¡œ ë°”ê¾¼ë¶€ë¶„ ##########################################
# exercise_llm.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from datetime import datetime
from typing import Dict, Any, Optional, List
from pymongo import MongoClient
import numpy as np
import asyncio
import os
import re
import requests

# ===== Embedding (384-d fixed) =====
from sentence_transformers import SentenceTransformer
EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")

def embed(text: str) -> List[float]:
    return embed_model.encode(text, normalize_embeddings=True).tolist()

VECTOR_DIM = len(embed_model.encode("dim"))


# ============================
# ğŸ”¥ LLM = Ollama API
# ============================
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "qwen2.5:3b"


# ============================
# ğŸ”¥ LLM Wrapper (ChatML ê°•ì œ)
# ============================
async def llm_generate(messages):
    def _run():
        res = requests.post(
            OLLAMA_URL,
            json={
                "model": OLLAMA_MODEL,
                "messages": messages,
                "stream": False
            }
        )
        data = res.json()
        try:
            return data["message"]["content"].strip()
        except:
            return "LLM ì‘ë‹µ ì˜¤ë¥˜"
    return await asyncio.to_thread(_run)



# ===== FastAPI =====
app = FastAPI(title="MediBear LLM Server (Local Mongo + RAG)")

# ===== MongoDB =====
client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=500)
db = client["ai_coach"]
chat_col = db["chat_history"]
profile_col = db["profile"]


# ===== Input Models =====
class ChatInput(BaseModel):
    user_id: str
    message: str

class ChatWithAnalysisInput(BaseModel):
    user_id: str
    message: str
    analysis: Dict[str, Any]


# ===== Utilities =====
def cosine_similarity(a, b) -> float:
    a, b = np.asarray(a), np.asarray(b)
    na, nb = np.linalg.norm(a), np.linalg.norm(b)
    return float(np.dot(a, b) / (na * nb)) if na * nb != 0 else 0.0

def safe_get_vec(doc) -> Optional[List[float]]:
    vec = doc.get("embedding") or doc.get("vector")
    if isinstance(vec, list) and len(vec) == VECTOR_DIM:
        return vec
    return None

def clean_text_korean_only(text: str) -> str:
    return re.sub(r"[^ê°€-í£0-9\s\.\,\?\!]", "", text)


# ===== RAG =====
def build_rag_context(user_id: str, user_msg: str, topk: int = 3, mode: str = "auto") -> str:
    qvec = embed(user_msg)

    if mode == "exercise":
        query = {"user_id": user_id, "type": "exercise"}
    elif mode == "general":
        query = {"user_id": user_id, "type": "general"}
    else:
        query = {"user_id": user_id}

    history = list(
        chat_col.find(query)
               .sort("timestamp", -1)
               .limit(60)
    )

    scored = []
    for h in history:
        vec = safe_get_vec(h)
        if vec:
            cleaned = clean_text_korean_only(h["message"])
            if cleaned.strip():
                scored.append((cosine_similarity(qvec, vec), cleaned))

    if not scored:
        return ""

    scored.sort(key=lambda x: x[0], reverse=True)

    # >>> ë³€ê²½ë¨: RAGë¥¼ ChatML assistant íˆìŠ¤í† ë¦¬ í˜•ì‹ìœ¼ë¡œ ë˜í•‘
    rag_messages = []
    for _, msg in scored[:topk]:
        rag_messages.append(f"<|im_start|>assistant\n{msg}\n<|im_end|>")

    return "\n".join(rag_messages)



# ===== Prompt ì •ì˜ =====
SYSTEM_PROMPT_EXERCISE = (
    "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì´ë‹¤.\n"
    "ì‚¬ìš©ìê°€ ì–´ë–¤ ìš´ë™ì„ í•˜ê³  ìˆëŠ”ì§€ ì²« ì¤„ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•œë‹¤.\n"
    "ë¶„ì„ ìˆ˜ì¹˜ëŠ” ì–¸ê¸‰í•˜ì§€ ì•ŠëŠ”ë‹¤.\n"
    "ìˆ«ì, ê°ë„, cm, %, Â° ë“± ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€.\n"
    "í•œì, ì˜ì–´, ì „ë¬¸ìš©ì–´, ê³¼í•™ì  í‘œí˜„ ê¸ˆì§€.\n"
    "ìš´ë™ ë™ì‘ì€ ëŠë‚Œê³¼ ì²´ì¤‘ íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…í•œë‹¤.\n\n"

    "ğŸš« [ë§¤ìš° ì¤‘ìš”í•œ ê·œì¹™]\n"
    "ì‚¬ìš©ìê°€ 'ì•„í”„ë‹¤', 'í†µì¦', 'ì°Œë¦¿í•˜ë‹¤', 'ë»ê·¼í•˜ë‹¤', 'í˜ë“¤ë‹¤' ë“± í†µì¦ ê´€ë ¨ í‘œí˜„ì„ í•˜ë©´\n"
    "ê·¸ ë¶€ìœ„ë¡œ ìš´ë™ì„ ì ˆëŒ€ ì‹œí‚¤ì§€ ë§ê³  ì¦‰ì‹œ ì¤‘ë‹¨í•˜ë„ë¡ ì•ˆë‚´í•˜ë¼.\n"
    "ì•„í”ˆ ë¶€ìœ„ì— ìê·¹ì´ ê°€ëŠ” ìš´ë™, ìŠ¤íŠ¸ë ˆì¹­, ë²„í‹°ê¸°, íšŒì „ ë™ì‘ì„ ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ë§ë¼.\n"
    "ëŒ€ì‹  ë¬´ë¦¬ê°€ ì—†ëŠ” ëŒ€ì²´ ìš´ë™ì´ë‚˜ íœ´ì‹ì„ ê¶Œì¥í•˜ë¼.\n\n"

    "ì¶œë ¥ í˜•ì‹:\n"
    "â‘  ì§€ê¸ˆ í•˜ê³  ìˆëŠ” ìš´ë™ ì´ë¦„ + ìì„¸ ëŠë‚Œ ìš”ì•½ (1~2ë¬¸ì¥)\n"
    "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
    "â‘¢ ê°œì„ í•  ì  (â€¢ ë¶ˆë¦¿ 2~3ê°œ)\n"
    "â‘£ ì½”ì¹­ í (â€¢ ë¶ˆë¦¿ 3~5ê°œ, ëª…ë ¹í˜• 4~8ê¸€ì)\n"
    "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥)\n"
)


SYSTEM_PROMPT_GENERAL = (
    "ë„ˆëŠ” í•œêµ­ì–´ í—¬ìŠ¤ì¼€ì–´ ìƒë‹´ AIì´ë‹¤.\n"
    "ì ˆëŒ€ ìš´ë™ í…œí”Œë¦¿ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì§§ê³  ë”°ëœ»í•˜ê³  ë‹¨í˜¸í•˜ê²Œ ë‹µí•œë‹¤.\n"
    "ì˜ì–´, í•œì, ì „ë¬¸ìš©ì–´ ì‚¬ìš© ê¸ˆì§€.\n"
)



# ===== Persona ìš”ì•½ =====
async def update_persona_background(user_id: str):
    chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(15))
    if not chats:
        return

    text = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])

    summary = await llm_generate([
        {
            "role": "system",
            "content": "<|im_start|>system\nì‚¬ìš©ìì˜ ë§íˆ¬/í†µì¦ ê²½í–¥/ìš´ë™ ìŠµê´€ì„ í•œêµ­ì–´ë¡œ 5ì¤„ë¡œ ìš”ì•½í•˜ë¼.\n<|im_end|>"
        },
        {
            "role": "user",
            "content": f"<|im_start|>user\n{text}\n<|im_end|>"
        }
    ])
    profile_col.update_one({"user_id": user_id}, {"$set": {"persona": summary}}, upsert=True)



# ===== ë‹µë³€ ìƒì„± =====
async def generate_answer(user_id: str, user_msg: str, analysis: Dict[str, Any]):
    persona = profile_col.find_one({"user_id": user_id}) or {}
    persona_text = persona.get("persona", "")

    # ìš´ë™ëª… ì„¸íŒ…
    if analysis and "exercise" in analysis and "detected_exercise" not in analysis:
        analysis["detected_exercise"] = analysis["exercise"]

    is_exercise = bool(analysis and analysis.get("detected_exercise"))

    if is_exercise:
        exercise_name = analysis["detected_exercise"]
        user_msg = f"{exercise_name} ìš´ë™ ì¤‘: {user_msg}"

    mode = "exercise" if is_exercise else "general"
    rag = build_rag_context(user_id, user_msg, mode=mode)

    # ğŸ”¥ system prompt í•˜ë‚˜ì— persona + RAG ëª¨ë‘ í¬í•¨
    system_prompt = SYSTEM_PROMPT_EXERCISE if is_exercise else SYSTEM_PROMPT_GENERAL

    system_content = f"""
{system_prompt}

ğŸ‘¤ [ì‚¬ìš©ì ë§íˆ¬/ìŠµê´€ ìš”ì•½]
{persona_text}

ğŸ“š [ì‚¬ìš©ì ì´ì „ ëŒ€í™” ê¸°ë¡]
{rag}
"""

    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_msg},
    ]

    return await llm_generate(messages)



# ===== ì €ì¥ =====
def save_chat(user_id, message, response, embedding, analysis):
    chat_col.insert_one({
        "user_id": user_id,
        "message": message,
        "response": response,
        "embedding": embedding,
        "analysis": analysis or {},
        "timestamp": datetime.now(),
        "type": "exercise" if (analysis and analysis.get("detected_exercise")) else "general",
    })



# ===== Endpoints =====
@app.post("/chat")
async def chat_plain(data: ChatInput, background_tasks: BackgroundTasks):
    qvec = embed(data.message)
    answer = await generate_answer(data.user_id, data.message, analysis={})
    save_chat(data.user_id, data.message, answer, qvec, {})
    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)
    return {"answer": answer}


@app.post("/chat_with_analysis")
async def chat_with_analysis(data: ChatWithAnalysisInput, background_tasks: BackgroundTasks):
    user_msg = data.message if data.message and data.message.strip() else "ìš´ë™ ìì„¸ í”¼ë“œë°± ìš”ì²­"

    qvec = embed(user_msg)

    answer = await generate_answer(data.user_id, user_msg, data.analysis)

    save_chat(data.user_id, user_msg, answer, qvec, data.analysis)

    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)

    return {"answer": answer}

